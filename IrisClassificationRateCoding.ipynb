{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409c7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ExpNeuron as en\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import snntorch.functional as snnfunc\n",
    "import snntorch.spikegen as snngen\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b5c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\tdef __init__(self, \n",
    "\t\t\tbeta_first = 0.8, threshold_first = 0.7, membrane_zero_first = 0.2, membrane_min_first = 0.3,\n",
    "\t\t\tlearn_beta_first = False, learn_threshold_first = False, learn_membrane_min_first = False,\n",
    "\t\t\tbeta_second = 0.8, threshold_second = 0.5, membrane_zero_second = 0.2, membrane_min_second = 0.3,\n",
    "\t\t\tlearn_beta_second = False, learn_threshold_second = False, learn_membrane_min_second = False\n",
    "\t\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.fll = nn.Linear(4, 4, bias = False)\n",
    "\t\tself.fsl = en.ExpNeuron(beta = beta_first, threshold = threshold_first,\n",
    "\t\t\t\t\t\t\t\t   \tmembrane_zero= membrane_zero_first,\n",
    "\t\t\t\t\t\t\t\t\tmembrane_min = membrane_min_first,\n",
    "\t\t\t\t\t\t\t\t\tlearn_beta= learn_beta_first, \n",
    "\t\t\t\t\t\t\t\t\tlearn_threshold= learn_threshold_first,\n",
    "\t\t\t\t\t\t\t\t\tlearn_membrane_min= learn_membrane_min_first)\n",
    "\t\t\n",
    "\t\tself.sll = nn.Linear(4, 3, bias = False)\n",
    "\t\tself.ssl = en.ExpNeuron(beta = beta_second, threshold = threshold_second,\n",
    "\t\t\t\t\t\t\t\t   \tmembrane_zero= membrane_zero_second,\n",
    "\t\t\t\t\t\t\t\t\tmembrane_min = membrane_min_second,\n",
    "\t\t\t\t\t\t\t\t\tlearn_beta= learn_beta_second, \n",
    "\t\t\t\t\t\t\t\t\tlearn_threshold= learn_threshold_second,\n",
    "\t\t\t\t\t\t\t\t\tlearn_membrane_min= learn_membrane_min_second)\n",
    "\t\t\n",
    "\tdef forward(self, spk_input):\n",
    "\t\tmem1 = self.fsl.init_neuron()\n",
    "\t\tmem2 = self.ssl.init_neuron()\n",
    "\n",
    "\t\tspk_output = []\n",
    "\t\tfor step in range(spk_input.shape[1]):\n",
    "\t\t\tcur1 = self.fll(spk_input.to(torch.float32)[:, step])\n",
    "\t\t\tspk1, mem1 = self.fsl(cur1, mem1)\n",
    "\t\t\tcur2 = self.sll(spk1)\n",
    "\t\t\tspk2, mem2 = self.ssl(cur2, mem2)\n",
    "\t\t\t\n",
    "\t\t\tspk_output.append(spk2)\n",
    "\t\t\n",
    "\t\treturn torch.stack(spk_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bac1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../iris_folder/Iris.csv\"\n",
    "df = pd.read_csv(filepath_or_buffer= file_path, sep= \",\", header= 0)\n",
    "df.drop(\"Id\", axis= 1, inplace= True)\n",
    "\n",
    "# преобразование датафрейма\n",
    "species = df[\"Species\"].unique()\n",
    "\n",
    "# При присваивании происходит изменение типа данных на object\n",
    "for i in range(len(species)):\n",
    "\tdf.loc[df[\"Species\"] == species[i], \"Species\"] = i\n",
    "df[\"Species\"] = df[\"Species\"].astype(\"int\")\n",
    "\n",
    "columns_headers = df.columns\n",
    "\n",
    "# Не учитываю species\n",
    "for header in columns_headers[:-1]:\n",
    "\t# Нормализую знеачения\n",
    "\tdf[header] = df[header] / df[header].max()\n",
    "# создание датасета для обучения\n",
    "data = []\n",
    "num_steps = 100\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "for header in columns_headers:\n",
    "\tdata.append(torch.tensor(df[header].values))\n",
    "\n",
    "# Транспонируем тензор, чтобы иметь features и target каждого образца\n",
    "data = torch.stack(data, dim = 0).T\n",
    "\n",
    "trains = snngen.rate(data= data[:, :-1], num_steps= num_steps)\n",
    "\n",
    "# labels = snngen.targets_rate(data[:, -1], num_classes=3)\n",
    "labels = data[:, -1]\n",
    "# Возможно полная хрень. Требует проверки\n",
    "# Я уверен, что с осями какой-то косяк, так что придется переделывать при плохих результатах обучения\n",
    "trains = trains.permute(1, 0, 2)\n",
    "dataset = TensorDataset(trains, labels)\n",
    "train_data, test_data = random_split(dataset, [0.8, 0.2])\n",
    "train_data_loader = DataLoader(train_data, shuffle= True, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fded714",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "states = []\n",
    "size = 5\n",
    "\n",
    "params = [[torch.concatenate([torch.rand(3).clamp(min = 0.3), torch.Tensor([0.2])]) for i in range(2)] for _ in range(size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201df47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net numb: 0. Parameters: [tensor([0.6463, 0.4851, 0.5160, 0.3596]), tensor([0.3000, 0.5860, 0.8367, 0.0366])]\n",
      "Epoch 0, Loss: 0.0100\n",
      "accuracy : 0.97\n",
      "Epoch 1, Loss: 0.0267\n",
      "accuracy : 0.87\n",
      "Epoch 2, Loss: 0.1011\n",
      "accuracy : 0.90\n",
      "Epoch 3, Loss: 0.0411\n",
      "accuracy : 0.97\n",
      "Epoch 4, Loss: 0.0056\n",
      "accuracy : 0.87\n",
      "Epoch 5, Loss: 0.0100\n",
      "accuracy : 0.70\n",
      "Epoch 6, Loss: 0.0100\n",
      "accuracy : 0.93\n",
      "Epoch 7, Loss: 0.0444\n",
      "accuracy : 0.90\n",
      "Epoch 8, Loss: 0.1011\n",
      "accuracy : 0.87\n",
      "Epoch 9, Loss: 0.0856\n",
      "accuracy : 0.97\n",
      "Epoch 10, Loss: 0.3789\n",
      "accuracy : 0.93\n",
      "Epoch 11, Loss: 0.0533\n",
      "accuracy : 0.87\n",
      "Epoch 12, Loss: 0.0178\n",
      "accuracy : 0.93\n",
      "Epoch 13, Loss: 0.1678\n",
      "accuracy : 0.93\n",
      "Epoch 14, Loss: 0.1533\n",
      "accuracy : 0.97\n",
      "Epoch 15, Loss: 0.0078\n",
      "accuracy : 0.93\n",
      "Epoch 16, Loss: 0.0044\n",
      "accuracy : 0.93\n",
      "Epoch 17, Loss: 0.0511\n",
      "accuracy : 0.90\n",
      "Epoch 18, Loss: 0.3922\n",
      "accuracy : 0.87\n",
      "Epoch 19, Loss: 0.0267\n",
      "accuracy : 0.93\n",
      "Net numb: 1. Parameters: [tensor([0.4552, 0.7732, 0.8924, 0.7582]), tensor([0.3000, 0.8018, 0.3000, 0.0712])]\n",
      "Epoch 0, Loss: 0.2167\n",
      "accuracy : 0.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \t\toutputs \u001b[38;5;241m=\u001b[39m net(trns)\n\u001b[0;32m     27\u001b[0m \t\tloss \u001b[38;5;241m=\u001b[39m loss_fn(spk_out\u001b[38;5;241m=\u001b[39moutputs, targets\u001b[38;5;241m=\u001b[39m lbls\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m---> 28\u001b[0m \t\t\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \t\toptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(size):\n",
    "\tprint(f\"Net numb: {i}. Parameters: {params[i]}\")\n",
    "\tlrng_rt = 1e-2\n",
    "\n",
    "\tnet = Net(\n",
    "\t\t\tbeta_first = params[i][0][0], threshold_first = params[i][0][1], membrane_zero_first = params[i][0][2], membrane_min_first = params[i][0][3],\n",
    "\t\t\tlearn_beta_first = False, learn_threshold_first = False, learn_membrane_min_first = True,\n",
    "\t\t\tbeta_second = params[i][1][0], threshold_second = params[i][1][1], membrane_zero_second = params[i][1][2], membrane_min_second = params[i][1][3],\n",
    "\t\t\tlearn_beta_second = False, learn_threshold_second = False, learn_membrane_min_second = True\n",
    "\t\t)\n",
    "\t\n",
    "\n",
    "\toptim = torch.optim.AdamW(net.parameters(), lr = lrng_rt)\n",
    "\tloss_fn = snnfunc.loss.mse_count_loss(correct_rate=0.3, incorrect_rate= 0.1, num_classes=3)\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\n",
    "\t\tif epoch == 10: lrng_rt = 1e-3\n",
    "\n",
    "\t\tfor trns, lbls in train_data_loader:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\t\t\t\toutputs = net(trns)\n",
    "\n",
    "\t\t\t\tloss = loss_fn(spk_out=outputs, targets= lbls.to(torch.long))\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptim.step()\n",
    "\n",
    "\n",
    "\t\tprint((f\"Epoch {epoch}, Loss: {loss.item():.4f}\"))\n",
    "\n",
    "\t\t# Testing accuracy\n",
    "\t\tdata = []\n",
    "\t\tfor trn, lbl in test_data_loader:\n",
    "\t\t\t\tprediction = net(trn)\n",
    "\t\t\t\tprediction = torch.mean(input= prediction.unsqueeze(0), dim = 1)\n",
    "\n",
    "\t\t\t\tdata.append([prediction.argmax().item(), lbl.item()])\n",
    "\t\t\t\t\n",
    "\t\tdata = torch.tensor(data)\n",
    "\t\taccurate = data[(data[:, 0] == data[:, 1]), 0].numel()\n",
    "\t\taccuracy = accurate/data.shape[0]\n",
    "\n",
    "\t\tif accuracy >= 0.9:\n",
    "\t\t\tstates.append(net.state_dict())\n",
    "\n",
    "\t\tprint((f\"accuracy : {accuracy:.2f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da97d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n",
      "accuracy : 0.90\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for state in states:\n",
    "    load_net = Net()\n",
    "    load_net.load_state_dict(state)\n",
    "\n",
    "    # Testing accuracy\n",
    "    data = []\n",
    "    for trn, lbl in test_data_loader:\n",
    "            prediction = load_net(trn)\n",
    "            prediction = torch.mean(input= prediction.unsqueeze(0), dim = 1)\n",
    "\n",
    "            data.append([prediction.argmax().item(), lbl.item()])\n",
    "            \n",
    "    data = torch.tensor(data)\n",
    "    accurate = data[(data[:, 0] == data[:, 1]), 0].numel()\n",
    "    accuracy = accurate/data.shape[0]\n",
    "\n",
    "    print((f\"accuracy : {accuracy:.2f}\"))\n",
    "    acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29614202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\glebm\\AppData\\Local\\Temp\\ipykernel_18336\\2185784947.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = torch.tensor(acc)\n"
     ]
    }
   ],
   "source": [
    "acc = torch.tensor(acc)\n",
    "max_acc_idx = torch.argmax(acc, dim = 0)\n",
    "\n",
    "# torch.save(states[max_acc_idx], \"Iris_ExpNeuron_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a62e6f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.93\n"
     ]
    }
   ],
   "source": [
    "load_net = Net()\n",
    "load_net.load_state_dict(torch.load(\"Iris_ExpNeuron_weights.pth\"))\n",
    "\n",
    "\n",
    "# Testing accuracy\n",
    "data = []\n",
    "for trn, lbl in test_data_loader:\n",
    "        prediction = load_net(trn)\n",
    "        prediction = torch.mean(input= prediction.unsqueeze(0), dim = 1)\n",
    "\n",
    "        data.append([prediction.argmax().item(), lbl.item()])\n",
    "        \n",
    "data = torch.tensor(data)\n",
    "accurate = data[(data[:, 0] == data[:, 1]), 0].numel()\n",
    "accuracy = accurate/data.shape[0]\n",
    "\n",
    "print((f\"accuracy : {accuracy:.2f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdde3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for par, val in net.named_parameters():\n",
    "#     print(par, val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
