{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612bac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import torch.nn.functional as nnfunc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ExpNeuron as en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241915c5",
   "metadata": {},
   "source": [
    "В данной модели используется следующая формула для вычисления мембранного потенциала:\n",
    "Я реализую численное решение для следующего уравнения: $U(t) = U_0(t)\\exp(-\\beta t + I(t))$\n",
    "\n",
    "Для того, чтобы численно решить данное уравнение его нужно сначала продифференцировать.\n",
    "Получим: $\\frac {dU(t)}{dt} = U(t)(-\\beta + \\frac {dI(t)}{dt})$\n",
    "\n",
    "$I(t)$ не дифференцируема по времени, так как она принимает дискретные значения. Я решил вместо $\\frac {dI(t)}{dt}$  использовать вес $w$ этого тока.\n",
    "Формула численного решения диффуравнения получается: $U(t + 1) = U(t) + U(t)(-\\beta + wI(t))\\Delta t$\n",
    "\n",
    "Если $U(t) \\geq threshold$, то $U_{0} = U_{min}$   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492224f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "\tdef __init__(self, beta, threshold, membrane_min, membrane_zero):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.lnr = nn.Linear(1, 1, bias= False)\n",
    "\t\tself.nrn = en.ExpNeuron(\n",
    "\t\t\tbeta= beta,\n",
    "\t\t\tthreshold= threshold,\n",
    "\t\t\tmembrane_zero= membrane_zero,\n",
    "\t\t\tmembrane_min= membrane_min,\n",
    "\t\t\tlearn_beta= True,\n",
    "\t\t\tlearn_membrane_min= True,\n",
    "\t\t\tlearn_threshold= True)\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self, spk_input):\n",
    "\t\tmem = self.nrn.init_neuron()\n",
    "\t\tspk_outpt = []\n",
    "\t\tsteps = spk_input.shape[1]\n",
    "\n",
    "\t\tfor step in range(steps):\n",
    "\t\t\tcurl = self.lnr(spk_input[:, step])\n",
    "\t\t\tspk, mem = self.nrn(curl, mem)\n",
    "\t\t\tspk_outpt.append(spk)\n",
    "\n",
    "\t\tspk_outpt = torch.stack(spk_outpt, dim = 1) \n",
    "\t\treturn spk_outpt\n",
    "\t\n",
    "\t\n",
    "def gen_spike_train(lambda_, num_steps):\n",
    "\tspike_train = torch.tensor([1 if lambda_ > element else 0 for element in torch.rand(num_steps)], dtype=torch.float32)\n",
    "\treturn spike_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4accb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0.95\n",
    "steps = 100\n",
    "samples = 150\n",
    "test_samples = 100\n",
    "eps = 5e-2\n",
    "btch_sz = 5\n",
    "\n",
    "\n",
    "gnrtr = torch.Generator().manual_seed(0)\n",
    "\n",
    "# Generating data in (a, b))\n",
    "a = 0.8\n",
    "b = 1\n",
    "if a > 0.8:\n",
    "\tlow_lambda_samples = int(samples * 0.3)\n",
    "\tlow_lambda_arr = torch.ones(low_lambda_samples).uniform_(0, 0.5)\n",
    "\tlambd_arr = torch.cat((torch.ones(samples - low_lambda_samples).uniform_(a, b), low_lambda_arr))\n",
    "else:\n",
    "\tlambd_arr = torch.ones(samples).uniform_(a, b)\n",
    "\n",
    "test_lambd_arr = torch.ones(test_samples).uniform_(0, 1)\n",
    "\n",
    "test_labels = ((abs(test_lambd_arr - Lambda) <= eps ) | (test_lambd_arr > Lambda)).float().unsqueeze(1)\n",
    "labels = ((abs(lambd_arr - Lambda) <= eps ) | (lambd_arr > Lambda)).float().unsqueeze(1)\n",
    "\n",
    "test_trains = torch.stack([gen_spike_train(lambd.item(), steps) for lambd in test_lambd_arr])\n",
    "trains = torch.stack([gen_spike_train(lambd.item(), steps) for lambd in lambd_arr])\n",
    "\n",
    "# train_data, test_data = random_split(TensorDataset(trains, labels), [samples * 80 // 100, samples * 20 // 100])\n",
    "train_data = TensorDataset(trains, labels)\n",
    "train_dataldr = DataLoader(dataset= train_data, batch_size= btch_sz)\n",
    "\n",
    "test_data = TensorDataset(test_trains, test_labels) \n",
    "test_dataldr = DataLoader(dataset = test_data, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e43e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.tensor(0.3)\n",
    "threshold = torch.tensor(0.9)\n",
    "membrane_min = torch.tensor(0.4)\n",
    "membrane_zero = torch.tensor(0.3)\n",
    "\n",
    "lrng_rt = 1e-2\n",
    "\n",
    "epochs = 20\n",
    "neuron = Neuron(beta = beta, threshold = threshold, membrane_min = membrane_min, membrane_zero = membrane_zero)\n",
    "optim = torch.optim.Adam(neuron.parameters(), lr = lrng_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e04ddca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m spike_cnt \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m nnfunc\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m spike_cnt, target\u001b[38;5;241m=\u001b[39m lbls, reduction\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ограничение weight\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\glebk\\python3.9\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glebk\\python3.9\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glebk\\python3.9\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\glebk\\python3.9\\lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BCE - функция потерь\n",
    "for epoch in range(epochs):\n",
    "\t\tfor trns, lbls in train_dataldr:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\t\t\t\toutputs = neuron(trns.unsqueeze(-1))\n",
    "\t\t\t\tspike_cnt = outputs.mean(dim=1)\n",
    "\n",
    "\t\t\t\tloss = nnfunc.binary_cross_entropy(input= spike_cnt, target= lbls, reduction= \"mean\")\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptim.step()\n",
    "\n",
    "\t\t\t\t# Ограничение weight\n",
    "\t\t\t\tneuron.lnr.weight.data.clamp_(min=0.1)\n",
    "\n",
    "\t\tif epoch % 5 == 0:\n",
    "\t\t\tprint((f\"Epoch {epoch}, Loss: {loss.item():.4f}, \"\n",
    "\t\t\t\tf\"Beta: {neuron.nrn.beta.item():.4f}, Threshold: {neuron.nrn.threshold.item():.4f}, \"\n",
    "\t\t\t\tf\"Membrane_min: {neuron.nrn.membrane_min.item():.4f}, Linear weight: {neuron.lnr.weight.item():.4f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cd3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.85, recall: 0.92, accuracy : 0.97\n",
      "\n",
      "true_positive: 11,\n",
      "true_negative: 86,\n",
      "false_negative: 1,\n",
      "false_positive: 2\n",
      "f1 metric: 0.880\n"
     ]
    }
   ],
   "source": [
    "\t\t\n",
    "# Testing accuracy\n",
    "data = []\n",
    "for trn, lbl in test_dataldr:\n",
    "\t\tprediction = neuron(trn).mean(dim=1).item()\n",
    "\t\tdata.append([float(prediction > 0.5), lbl.item()])\n",
    "\t\t# print((f\"pred: {prediction:.2f}, predicted: {prediction > 0.5}, \"\n",
    "\t\t#        f\"true: {lbl}\"))\n",
    "data = torch.tensor(data)\n",
    "true_positive = data[(data[:, 0] == 1) & (data[:, 1] == 1), 0].numel()\n",
    "false_positive = data[(data[:, 0] == 1) & (data[:, 1] == 0), 0].numel()\n",
    "true_negative = data[(data[:, 0] == 0) & (data[:, 1] == 0), 0].numel()\n",
    "false_negative = data[(data[:, 0] == 0) & (data[:, 1] == 1), 0].numel()\n",
    "\n",
    "# Counting metrics\n",
    "if true_positive and true_negative: \n",
    "\t\tprecision = true_positive / (true_positive + false_positive)\n",
    "\t\trecall = true_positive / (true_positive + false_negative)\n",
    "\t\taccuracy = (true_positive + true_negative) / (true_positive + true_negative \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t+ false_positive + false_negative)\n",
    "\n",
    "\t\tprint((f\"precision: {precision:.2f}, recall: {recall:.2f}, accuracy : {accuracy:.2f}\"))\n",
    "\t\tprint((f\"\\ntrue_positive: {true_positive},\\n\"\n",
    "\t\t\t\tf\"true_negative: {true_negative},\\nfalse_negative: {false_negative},\\n\"\n",
    "\t\t\t\tf\"false_positive: {false_positive}\"))\n",
    "else:\n",
    "\t\tprint((f\"Low accuracy of the model.\\ntrue_positive: {true_positive},\\n\"\n",
    "\t\t\t\tf\"true_negative: {true_negative},\\nfalse_negative: {false_negative},\\n\"\n",
    "\t\t\t\tf\"false_positive: {false_positive}\"))\n",
    "\t\n",
    "if precision and recall:\n",
    "\t\tf1 = 2 * precision * recall / (precision + recall)\n",
    "\t\tprint(f\"f1 metric: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnr.weight Parameter containing:\n",
      "tensor([[0.9573]], requires_grad=True)\n",
      "nrn.threshold Parameter containing:\n",
      "tensor(0.4584, requires_grad=True)\n",
      "nrn.beta Parameter containing:\n",
      "tensor(0.7475, requires_grad=True)\n",
      "nrn.membrane_min Parameter containing:\n",
      "tensor(1.1698, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par, val in neuron.named_parameters():\n",
    "\tprint(par, val)\n",
    "\n",
    "# выгрузка параметров\n",
    "# torch.save(neuron.state_dict(), \"neuron_weights.pth\")\n",
    "\n",
    "# для загрузки\n",
    "# neuron.load_state_dict(torch.load(\"neuron_weights.pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
