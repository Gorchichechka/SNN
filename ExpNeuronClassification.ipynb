{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612bac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import torch.nn.functional as nnfunc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ExpNeuron as en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492224f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "\tdef __init__(self, beta, threshold, membrane_min):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.nrn = en.ExpNeuron(\n",
    "\t\t\tbeta= beta,\n",
    "\t\t\tthreshold= threshold,\n",
    "\t\t\tmembrane_zero= membrane_min,\n",
    "\t\t\tmembrane_min= membrane_min,\n",
    "\t\t\tlearn_beta= True,\n",
    "\t\t\tlearn_membrane_min= True,\n",
    "\t\t\tlearn_threshold= True)\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self, spk_input):\n",
    "\t\tmem = self.nrn.init_neuron()\n",
    "\t\tspk_outpt = []\n",
    "\t\tsteps = spk_input.shape[1]\n",
    "\n",
    "\t\tfor step in range(steps):\n",
    "\t\t\tspk, mem = self.nrn(spk_input[:, step])\n",
    "\t\t\tspk_outpt.append(spk)\n",
    "\n",
    "\t\tspk_outpt = torch.stack(spk_outpt, dim = 1) \n",
    "\n",
    "\t\treturn spk_outpt\n",
    "\t\n",
    "\t\n",
    "def gen_spike_train(lambda_, num_steps):\n",
    "\tspike_train = torch.tensor([1 if lambda_ > element else 0 for element in torch.rand(num_steps)], dtype=torch.float32)\n",
    "\treturn spike_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4accb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0.95\n",
    "steps = 100\n",
    "samples = 100\n",
    "test_samples = 200\n",
    "eps = 5e-2\n",
    "# не забыть вернуть\n",
    "btch_sz = 5\n",
    "\n",
    "\n",
    "gnrtr = torch.Generator().manual_seed(0)\n",
    "\n",
    "# Generating data in (a, b))\n",
    "a = 0.8\n",
    "b = 1\n",
    "if a > 0.8:\n",
    "\tlow_lambda_samples = int(samples * 0.3)\n",
    "\tlow_lambda_arr = torch.ones(low_lambda_samples).uniform_(0, 0.5)\n",
    "\tlambd_arr = torch.cat((torch.ones(samples - low_lambda_samples).uniform_(a, b), low_lambda_arr))\n",
    "else:\n",
    "\tlambd_arr = torch.ones(samples).uniform_(a, b)\n",
    "\n",
    "test_lambd_arr = torch.ones(test_samples).uniform_(0, 1)\n",
    "\n",
    "test_labels = ((abs(test_lambd_arr - Lambda) <= eps ) | (test_lambd_arr > Lambda)).float().unsqueeze(1)\n",
    "labels = ((abs(lambd_arr - Lambda) <= eps ) | (lambd_arr > Lambda)).float().unsqueeze(1)\n",
    "\n",
    "test_trains = torch.stack([gen_spike_train(lambd.item(), steps) for lambd in test_lambd_arr])\n",
    "trains = torch.stack([gen_spike_train(lambd.item(), steps) for lambd in lambd_arr])\n",
    "\n",
    "# train_data, test_data = random_split(TensorDataset(trains, labels), [samples * 80 // 100, samples * 20 // 100])\n",
    "train_data = TensorDataset(trains, labels)\n",
    "train_dataldr = DataLoader(dataset= train_data, batch_size= btch_sz)\n",
    "\n",
    "test_data = TensorDataset(test_trains, test_labels) \n",
    "test_dataldr = DataLoader(dataset = test_data, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e43e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.tensor(0.5)\n",
    "threshold = torch.tensor(1.0)\n",
    "membrane_min = torch.tensor(0.5)\n",
    "\n",
    "lrng_rt = 1e-3\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "neuron = Neuron(beta = beta, threshold = threshold, membrane_min = membrane_min)\n",
    "optim = torch.optim.Adam(neuron.parameters(), lr = lrng_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e04ddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) 0 tensor(0.5000)\n",
      "correcting shape\n",
      "tensor([1., 0., 1., 0., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 2., 1., 2., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 3., 1., 3., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 0.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 2., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 3., 2., 1., 4.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 4., 3., 1., 5.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 5., 1., 1., 6.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 1., 1., 1., 7.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 0.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 2., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 3., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 4., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 5., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([2., 6., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([3., 7., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 0.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 2., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 0.]) tensor([1., 1., 3., 1., 4.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 1., 1., 1., 5.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 0., 1.]) tensor([1., 2., 1., 1., 6.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 3., 1., 1., 7.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 4., 1., 2., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 5., 1., 3., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 0., 1.]) tensor([2., 6., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([3., 7., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 2., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 3., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([2., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([3., 1., 2., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 0., 1., 1.]) tensor([1., 1., 3., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 0., 1.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([2., 1., 2., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 1.]) tensor([3., 1., 3., 2., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 2., 1., 3., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 3., 2., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 1., 3., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 0.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 1., 2., 1., 4.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 3., 1., 5.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 0.]) tensor([2., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 1.]) tensor([3., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 2., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 3., 2., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 1., 3., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 0., 1.]) tensor([2., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([3., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 2., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 3., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 2., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 3., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 0.]) tensor([1., 1., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([2., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([3., 1., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([4., 1., 1., 1., 4.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([5., 2., 1., 1., 5.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 3., 1., 1., 6.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 7.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([2., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([3., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 0., 1., 1., 0.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 0., 1.]) tensor([2., 1., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([3., 2., 1., 1., 4.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([4., 3., 1., 2., 5.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([5., 1., 1., 3., 6.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 1., 1., 1., 7.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 8.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 9.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 0.]) tensor([ 1.,  1.,  1.,  1., 10.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([ 1.,  1.,  1.,  1., 11.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 0., 1., 1., 1.]) tensor([ 1.,  1.,  2.,  1., 12.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([0., 1., 1., 1., 1.]) tensor([ 1.,  1.,  3.,  1., 13.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([ 1.,  2.,  1.,  1., 14.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([ 2.,  3.,  1.,  1., 15.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([3., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 0.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 1., 1.]) tensor([1., 1., 1., 1., 2.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 1., 0., 1.]) tensor([1., 1., 1., 1., 3.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n",
      "tensor([1., 1., 0., 1., 1.]) tensor([1., 1., 1., 1., 1.]) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [5]] is at version 200; expected version 199 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# При Lambda > 0.9 и Lambda < 0.1 достоверность результатов падает\u001b[39;00m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m nnfunc\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m spike_cnt, target\u001b[38;5;241m=\u001b[39m lbls\u001b[38;5;241m.\u001b[39msqueeze(), reduction\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# В данной модели beta принимает значения [0, 1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glebm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [5]] is at version 200; expected version 199 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\t\tfor trns, lbls in train_dataldr:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\t\t\toutputs = neuron(trns)\n",
    "\t\t\t\tspike_cnt = outputs.mean(dim=1)\n",
    "\t\t\t\t# При Lambda > 0.9 и Lambda < 0.1 достоверность результатов падает\n",
    "\n",
    "\t\t\t\tloss = nnfunc.binary_cross_entropy(input= spike_cnt, target= lbls.squeeze(), reduction= \"mean\")\n",
    "\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\t\toptim.step()\n",
    "\t\n",
    "\t\t\t\t# В данной модели beta принимает значения [0, 1]\n",
    "\t\t\t\tif beta.item() > 1:\n",
    "\t\t\t\t\tbeta = torch.tensor(0.999)\n",
    "\t\t\t\tif beta.item() < 0:\n",
    "\t\t\t\t\tbeta = torch.tensor(0.01)\n",
    "\n",
    "\t\tif epoch % 5 == 0:\n",
    "\t\t\tprint((f\"Epoch {epoch}, Loss: {loss.item():.4f}, \"\n",
    "\t\t\t\tf\"Beta: {neuron.nrn.beta.item():.4f}, Threshold: {neuron.nrn.threshold.item():.4f}\"))\n",
    "\t\t\n",
    "\n",
    "\n",
    "# Testing accuracy\n",
    "data = []\n",
    "beta = torch.tensor(0.3234)\n",
    "threshold = torch.tensor(0.6971)\n",
    "for trn, lbl in test_dataldr:\n",
    "\t\tprediction = neuron(trn).mean(dim=1).item()\n",
    "\t\tdata.append([float(prediction > 0.5), lbl.item()])\n",
    "\t\t# print((f\"pred: {prediction:.2f}, predicted: {prediction > 0.5}, \"\n",
    "\t\t#        f\"true: {lbl}\"))\n",
    "data = torch.tensor(data)\n",
    "true_positive = data[(data[:, 0] == 1) & (data[:, 1] == 1), 0].numel()\n",
    "false_positive = data[(data[:, 0] == 1) & (data[:, 1] == 0), 0].numel()\n",
    "true_negative = data[(data[:, 0] == 0) & (data[:, 1] == 0), 0].numel()\n",
    "false_negative = data[(data[:, 0] == 0) & (data[:, 1] == 1), 0].numel()\n",
    "\n",
    "if true_positive and true_negative: \n",
    "\t\tprecision = true_positive / (true_positive + false_positive)\n",
    "\t\trecall = true_positive / (true_positive + false_negative)\n",
    "\t\taccuracy = (true_positive + true_negative) / (true_positive + true_negative \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t+ false_positive + false_negative)\n",
    "\n",
    "\t\tprint((f\"precision: {precision:.2f}, recall: {recall:.2f}, accuracy : {accuracy:.2f}\"))\n",
    "\t\tprint((f\"\\ntrue_positive: {true_positive},\\n\"\n",
    "\t\t\t\tf\"true_negative: {true_negative},\\nfalse_negative: {false_negative},\\n\"\n",
    "\t\t\t\tf\"false_positive: {false_positive}\"))\n",
    "else:\n",
    "\t\tprint((f\"Low accuracy of the model.\\ntrue_positive: {true_positive},\\n\"\n",
    "\t\t\t\tf\"true_negative: {true_negative},\\nfalse_negative: {false_negative},\\n\"\n",
    "\t\t\t\tf\"false_positive: {false_positive}\"))\n",
    "\t\n",
    "if precision and recall:\n",
    "\t\tf1 = 2 * precision * recall / (precision + recall)\n",
    "\t\tprint(f\"f1 metric: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrn.threshold Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "nrn.beta Parameter containing:\n",
      "tensor(0.5000, requires_grad=True)\n",
      "nrn.membrane_min Parameter containing:\n",
      "tensor(0.5000, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par, val in neuron.named_parameters():\n",
    "\tprint(par, val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
